{"title": "Cache Implementations in C# .NET", "content": "<p>One of the most commonly used patterns in software development is\u00a0<strong>Caching</strong>. It\u2019s a simple, but a very effective concept. The idea is to reuse operation results. When performing a heavy operation, we will save the result in our\u00a0<strong>cache container</strong>. The next time that we need that result, we will pull it from the cache container, instead of performing the heavy operation again.</p>\n<p>For example, to get a person\u2019s Avatar you might need a trip to the database. Instead of performing that trip every time, we will save that Avatar in the cache, pulling it from memory every time you need it.</p>\n<p>Caching works great for data that changes infrequently. Or even better, never changes. Data that constantly changes, like the current machine\u2019s time shouldn\u2019t be cached or you will get wrong results.</p>\n<h2>In-process Cache, Persistant in-process Cache, and Distributed Cache</h2>\n<p>There are 3 types of caches:</p>\n<ul>\n<li><strong>In-Memory Cache</strong>\u00a0is used for when you want to implement cache in a single process. When the process dies, the cache dies with it. If you\u2019re running the same process on several servers, you will have a separate cache for each server.</li>\n<li><strong>Persistent in-process Cache</strong>\u00a0is when you back up your cache outside of process memory. It might be in a file, or in a database. This is more difficult, but if your process is restarted, the cache is not lost. Best used when getting the cached item is expansive, and your process tends to restart a lot.</li>\n<li><strong>Distributed Cache</strong>\u00a0is when you want to have shared cache for several machines. Usually, it will be several servers. With a distributed cache, it is stored in an external service. This means if one server saved a cache item, other servers can use it as well. Services like\u00a0<a href=\"https://redis.io/\">Redis</a>\u00a0are great for this.</li>\n</ul>\n<p>We\u2019re going to talk just about\u00a0<strong>in-process cache</strong>.</p>\n<h2>Naive Implementation</h2>\n<p>Let\u2019s create a very simple cache implementation in C#:</p>\n<pre><code data-enlighter-language=\"null\">public class NaiveCache&lt;TItem&gt;\r\n{\r\n    Dictionary&lt;object, TItem&gt; _cache = new Dictionary&lt;object, TItem&gt;();\r\n \r\n    public TItem GetOrCreate(object key, Func&lt;TItem&gt; createItem)\r\n    {\r\n        if (!_cache.ContainsKey(key))\r\n        {\r\n            _cache[key] = createItem();\r\n        }\r\n        return _cache[key];\r\n    }\r\n}\r\n</code></pre>\n<p>Usage:</p>\n<div id=\"crayon-5ce7d6f4e6f14459952846\" class=\"crayon-syntax crayon-theme-github crayon-font-consolas crayon-os-pc print-yes notranslate crayon-wrapped\" data-settings=\" no-popup minimize scroll-mouseover wrap\">\n<div class=\"crayon-plain-wrap\">\n<pre><code data-enlighter-language=\"null\">var _avatarCache = new NaiveCache&lt;byte[]&gt;();\r\n// ...\r\nvar myAvatar = _avatarCache.GetOrCreate(userId, () =&gt; _database.GetAvatar(userId));</code></pre>\n<p>This simple code solves a crucial problem. To get a user\u2019s avatar, only the first request will actually perform a trip to the database. The avatar data (<code>byte[]</code>) is then saved in process memory. All following requests for the avatar will be pulled from memory, saving time and resources.</p>\n<p>But, as most things in programming, nothing is so simple. The above solution is not good for a number of reasons. For one thing, this implementation is\u00a0<strong>not thread-safe</strong>. Exceptions can occur when used from multiple threads. Besides that, cached items will stay in memory forever, which is actually very bad.</p>\n<p><strong>Here\u2019s why we should be removing items from Cache:</strong></p>\n<ol>\n<li>Cache can take up a lot of memory, eventually leading to an out-of-memory exceptions and crashes.</li>\n<li>High memory consumption can lead to\u00a0<strong>GC Pressure</strong>\u00a0(aka Memory Pressure). In this state, the garbage collector works more than it should, hurting performance.</li>\n<li>Cache might need to be refreshed if the data changes. Our caching infrastructure should support that ability.</li>\n</ol>\n<p>To handle these problems, cache frameworks have\u00a0<strong>Eviction policies</strong>\u00a0(aka\u00a0<strong>Removal policies</strong>). These are rules to have items removed from cache according to some logic. Common eviction policies are:</p>\n<ul>\n<li><strong>Absolute Expiration</strong>\u00a0policy will remove an item from cache after a fixed amount of time, no matter what.</li>\n<li><strong>Sliding Expiration</strong>\u00a0policy will remove an item from cache if it wasn\u2019t\u00a0<strong>accessed</strong>\u00a0in a fixed amound of time. So if I set the expiration to 1 minute, the item will keep staying in cache as long as I use it every 30 seconds. Once I don\u2019t use it for longer than a minute, the item is evicted.</li>\n<li><strong>Size Limit</strong>\u00a0policy will limit the cache memory size.</li>\n</ul>\n<p>Now that we know what we need, let\u2019s continue on to better solutions.</p>\n<h3>Better Solutions</h3>\n<p>To my great dismay as a blogger, Microsoft already created a wonderful cache implementation. This deprived me the pleasure of creating a similar implementation myself, but at least I have less work writing this blog post.</p>\n<p>I\u2019ll show you Microsoft\u2019s solution, how to effectively use it, and then how to improve it in some scenarios.</p>\n<h3>System.Runtime.Caching/MemoryCache vs Microsoft.Extensions.Caching.Memory</h3>\n<p>Microsoft has 2 solutions 2 different NuGet packages for caching. Both are great. As per Microsoft\u2019s\u00a0<a href=\"https://docs.microsoft.com/en-us/aspnet/core/performance/caching/memory?view=aspnetcore-2.2#systemruntimecachingmemorycache\">recommendation</a>, prefer using\u00a0<code>Microsoft.Extensions.Caching.Memory</code>\u00a0because it integrates better with Asp. NET Core. It can be\u00a0<a href=\"https://docs.microsoft.com/en-us/aspnet/core/performance/caching/memory?view=aspnetcore-2.2#using-imemorycache\">easily injected</a>\u00a0into Asp .NET Core\u2019s dependency injection mechanism.</p>\n<p>Here\u2019s a basic example with\u00a0<code>Microsoft.Extensions.Caching.Memory</code>:</p>\n<pre><code data-enlighter-language=\"null\">public class SimpleMemoryCache&lt;TItem&gt;\r\n{\r\n    private MemoryCache _cache = new MemoryCache(new MemoryCacheOptions());\r\n \r\n    public TItem GetOrCreate(object key, Func&lt;TItem&gt; createItem)\r\n    {\r\n        TItem cacheEntry;\r\n        if (!_cache.TryGetValue(key, out cacheEntry))// Look for cache key.\r\n        {\r\n            // Key not in cache, so get data.\r\n            cacheEntry = createItem();\r\n            \r\n            // Save data in cache.\r\n            _cache.Set(key, cacheEntry);\r\n        }\r\n        return cacheEntry;\r\n    }\r\n}\r\n\r\nUsage:\r\n\r\nvar _avatarCache = new SimpleMemoryCache&lt;byte[]&gt;();\r\n// ...\r\nvar myAvatar = _avatarCache.GetOrCreate(userId, () =&gt; _database.GetAvatar(userId));\r\n</code></pre>\n<p>This is very similar to my own\u00a0<code>NaiveCache</code>, so what changed? Well, for one thing, this is a\u00a0<strong>thread-safe</strong>implementation. You can safely call this from multiple threads at once.</p>\n<p>The second thing thing is the\u00a0<code>MemoryCache</code>\u00a0allows for all the\u00a0<strong>eviction policies</strong>\u00a0we talked about before. Here\u2019s an example:</p>\n<h3>IMemoryCache with eviction policies:</h3>\n<pre><code data-enlighter-language=\"null\">public class MemoryCacheWithPolicy&lt;TItem&gt;\r\n{\r\n    private MemoryCache _cache = new MemoryCache(new MemoryCacheOptions()\r\n    {\r\n        SizeLimit = 1024\r\n    });\r\n \r\n    public TItem GetOrCreate(object key, Func&lt;TItem&gt; createItem)\r\n    {\r\n        TItem cacheEntry;\r\n        if (!_cache.TryGetValue(key, out cacheEntry))// Look for cache key.\r\n        {\r\n            // Key not in cache, so get data.\r\n            cacheEntry = createItem();\r\n \r\n            var cacheEntryOptions = new MemoryCacheEntryOptions()\r\n             .SetSize(1)//Size amount\r\n             //Priority on removing when reaching size limit (memory pressure)\r\n                .SetPriority(CacheItemPriority.High)\r\n                // Keep in cache for this time, reset time if accessed.\r\n                .SetSlidingExpiration(TimeSpan.FromSeconds(2))\r\n                // Remove from cache after this time, regardless of sliding expiration\r\n                .SetAbsoluteExpiration(TimeSpan.FromSeconds(10));\r\n \r\n            // Save data in cache.\r\n            _cache.Set(key, cacheEntry, cacheEntryOptions);\r\n        }\r\n        return cacheEntry;\r\n    }\r\n}</code></pre>\n<p>Let\u2019s analyze the new additions:</p>\n<ol>\n<li><code>SizeLimit</code>\u00a0was added in\u00a0<code>MemoryCacheOptions</code>. This adds a size-based policy to our cache container. Size doesn\u2019t have a unit. Instead, we need to set the size amount on each cache entry. In this case, we set the amount to 1 each time with\u00a0<code>SetSize(1)</code>. This means that the cache is limited to 1024 items.</li>\n<li>When we reach the size limit, which cache item should be removed? You can actually set priority with\u00a0<code>.SetPriority(CacheItemPriority.High)</code>. The levels are\u00a0<strong>Low, Normal, High,</strong>\u00a0and\u00a0<strong>NeverRemove</strong>.</li>\n<li><code>SetSlidingExpiration(TimeSpan.FromSeconds(2))</code>\u00a0was added, which sets\u00a0<strong>sliding expiration</strong>\u00a0to 2 seconds. That means if an item was not accessed in over 2 seconds it will be removed.</li>\n<li><code>SetAbsoluteExpiration(TimeSpan.FromSeconds(10))</code>\u00a0was added, which sets\u00a0<strong>absolute expiration</strong>\u00a0to 10 seconds. This means than the item will be evicted within 10 seconds if it wasn\u2019t already.</li>\n</ol>\n<p>In addition to the options in the example, you can also set a\u00a0<code>RegisterPostEvictionCallback</code>\u00a0delegate, which will be called when an item is evicted.</p>\n<p>That\u2019s a pretty comprehensive feature set. It makes you wonder if there\u2019s even anything else to add. There are actually a couple of things.</p>\n<h2>Problems and Missing features</h2>\n<p>There are a couple of important missing pieces in this implementation.</p>\n<ol>\n<li>While you can set the size-limit, the caching doesn\u2019t actually monitor gc pressure. If we did monitor it, we could tighten policies when the pressure is high, and loosen up policies when the pressure is low.</li>\n<li>When requesting the same item with multiple threads at the same time, the requests don\u2019t wait for the first one to finish. The item will be created multiple times. For example, let\u2019s say we are caching the Avatar, and getting an avatar from the database takes 10 seconds. If we request an avatar 2 seconds after the first request, it will check if the avatar is cached (it isn\u2019t yet), and start another trip to the database.</li>\n</ol>\n<p><strong>As for the first problem</strong>\u00a0of gc pressure: It\u2019s possible to monitor GC pressure with several techniques and heuristics. This blog post is not about that, but you can read my article\u00a0<a href=\"https://michaelscodingspot.com/find-fix-and-avoid-memory-leaks-in-c-net-8-best-practices/\">Find, Fix, and Avoid Memory Leaks in C# .NET: 8 Best Practices</a>\u00a0to learn of some helpful methods.</p>\n<p><strong>The second problem</strong>\u00a0is easier to solve. In fact, here\u2019s an implementation of\u00a0<code>MemoryCache</code>\u00a0that solves it entirely:</p>\n<pre><code data-enlighter-language=\"null\">public class WaitToFinishMemoryCache&lt;TItem&gt;\r\n{\r\n    private MemoryCache _cache = new MemoryCache(new MemoryCacheOptions());\r\n    private ConcurrentDictionary&lt;object, SemaphoreSlim&gt; _locks = new ConcurrentDictionary&lt;object, SemaphoreSlim&gt;();\r\n \r\n    public async Task&lt;TItem&gt; GetOrCreate(object key, Func&lt;Task&lt;TItem&gt;&gt; createItem)\r\n    {\r\n        TItem cacheEntry;\r\n \r\n        if (!_cache.TryGetValue(key, out cacheEntry))// Look for cache key.\r\n        {\r\n            SemaphoreSlim mylock = _locks.GetOrAdd(key, k =&gt; new SemaphoreSlim(1, 1));\r\n \r\n            await mylock.WaitAsync();\r\n            try\r\n            {\r\n                if (!_cache.TryGetValue(key, out cacheEntry))\r\n                {\r\n                    // Key not in cache, so get data.\r\n                    cacheEntry = await createItem();\r\n                    _cache.Set(key, cacheEntry);\r\n                }\r\n            }\r\n            finally\r\n            {\r\n                mylock.Release();\r\n            }\r\n        }\r\n        return cacheEntry;\r\n    }\r\n}\r\n\r\n\r\nUsage:\r\n\r\n\r\nvar _avatarCache = new WaitToFinishMemoryCache&lt;byte[]&gt;();\r\n// ...\r\nvar myAvatar = \r\n await _avatarCache.GetOrCreate(userId, async () =&gt; await _database.GetAvatar(userId));</code></pre>\n<p>With this, when trying to get an item, if the same item is in the middle of being created by another thread, you will wait for the other to finish first. Then, you will get the already cached item created by the other thread.</p>\n<h3>Explanation of the code</h3>\n<p>This implementation locks the creation of an item. The lock is specific to the key. For example, if we\u2019re waiting to get Alex\u2019s Avatar, we can still get cached values of John or Sarah on another thread.</p>\n<p>The dictionary\u00a0<code>_locks</code>\u00a0stores all the locks. Regular locks don\u2019t work with\u00a0<code>async/await</code>, so we need to use\u00a0<a href=\"https://blog.cdemi.io/async-waiting-inside-c-sharp-locks/\"><code>SemaphoreSlim</code></a>.</p>\n<p>There are 2 checks to see if the value is already cached if (!_cache.TryGetValue(key, out cacheEntry)). The one inside the lock is the one that ensures there\u2019s a single creation. The one outside of the lock is for optimization.</p>\n<h3>When to use WaitToFinishMemoryCache</h3>\n<p>This implementation obviously has some overhead. Let\u2019s consider when it\u2019s even necessary.</p>\n<p>Use WaitToFinishMemoryCache when:</p>\n<ul>\n<li>When the creation time of an item has some sort of cost, and you want to minimize creations as much as possible.</li>\n<li>When the creation time of an item is very long.</li>\n<li>When the creation of an item has to be ensured to be done once per key.</li>\n</ul>\n<p><strong>Don\u2019t</strong>\u00a0use WaitToFinishMemoryCache when:</p>\n<ul>\n<li>There\u2019s no danger of multiple threads accessing the same cache item.</li>\n<li>You don\u2019t mind creating the item more than once. For example, if one extra trip to the database won\u2019t change much.</li>\n</ul>\n<h2>Summary</h2>\n<p>Caching is a very powerful pattern. It\u2019s also dangerous and has its own complexities. Cache too much and you can cause GC pressure. Cache too little and you can cause performance issues. Then, there\u2019s distributed caching, which is a whole new world to explore. That\u2019s software development for you, always something new to learn.</p>\n<p>I hope you enjoyed this post. If you\u2019re interested in memory management, my next article is going to be about the dangers of GC pressure and techniques to prevent it, so\u00a0<a href=\"https://michaelscodingspot.com/subscribe\">keep following</a>. Happy coding.</p>\n<p>Source : <a href=\"https://michaelscodingspot.com/cache-implementations-in-csharp-net/\">https://michaelscodingspot.com/cache-implementations-in-csharp-net/</a></p>\n</div>\n<div class=\"crayon-main\"></div>\n</div>\n", "slug": "cache-implementations-in-c-net", "date": 1558709951, "cats": [0, 13, 17, 15]}