{"title": "From bare-metal to Kubernetes", "content": "<h4><span style=\"color: #800000;\">I am impressed about this great journey and because i faced the same scenarios and followed the same steps I&#8217;m sharing this post from <a style=\"color: #800000;\" href=\"https://boxunix.com/post/bare_metal_to_kube/\" target=\"_blank\" rel=\"noopener noreferrer\">https://boxunix.com/post/bare_metal_to_kube/</a></span></h4>\n<p> </p>\n<div id=\"preamble\">\n<div class=\"sectionbody\">\n<div class=\"admonitionblock note\">\n<table>\n<tbody>\n<tr>\n<td class=\"icon\">\n<div class=\"title\"></div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p><a href=\"https://www.betabrand.com/\">Betabrand</a>\u00a0is a retail clothing company and crowdfunding platform, based in San Francisco.</p>\n</div>\n<div class=\"paragraph\">\n<p>The company designs, manufactures, and releases new products in limited quantities each week.</p>\n</div>\n</td>\n</tr>\n</tbody>\n</table>\n</div>\n<div class=\"paragraph\">\n<p>How migrating\u00a0<a href=\"https://www.betabrand.com/\">Betabrand</a>&#8216;s bare-metal infrastructure to a Kubernetes cluster hosted on Google Container Engine solved many engineering issues\u2014\u200bfrom hardware failures, to lack of scalability of our production services, complex configuration management and highly heterogeneous development-staging-production environments\u2014\u200band allowed us to achieve a reliable, available and scalable infrastructure.</p>\n</div>\n<div class=\"paragraph\">\n<p>This post will walk you through the many infrastructure changes and challenges Betabrand met from 2011 to 2018.</p>\n</div>\n<hr />\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"_early_infrastructure\">Early infrastructure</h2>\n<div class=\"sectionbody\">\n<div class=\"sect2\">\n<h3 id=\"_vps\">VPS</h3>\n<div class=\"paragraph\">\n<p>Betabrand\u2019s infrastructure has changed many times over the course of the 7 years I\u2019ve worked here.</p>\n</div>\n<div class=\"paragraph\">\n<p>In 2011, the year our CTO hired me, the website was hosted on a shared server with a Plesk interface and no root access (of course). Every newsletter send\u2014\u200bto at most a few hundred people\u2014\u200bwould bring the website to its knees and make it crawl, even completely unresponsive at times.</p>\n</div>\n<div class=\"paragraph\">\n<p>My first order of business became finding a replacement and move the website to its own dedicated server.</p>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_rackspace\">Rackspace</h3>\n<div class=\"paragraph\">\n<p>After a few days of online research, we settled on a VPS\u2014\u200b8GB RAM, 320GO Disk, 4 Virtual CPU, 150Mbps of bandwidth\u2014\u200bat Rackspace\u00a0. A few more days and we were live on our new infrastructure composed of\u2026 1 server; running your typical Linux, Apache, PHP, MySQL stack, with a hint of Memcached.</p>\n</div>\n<div class=\"paragraph\">\n<p>Unsurprisingly, this infrastructure quickly became obsolete.</p>\n</div>\n<div class=\"paragraph\">\n<p>Not only didn\u2019t it scale at all but, more importantly, every part of it was a Single Point Of Failure. Apache down? Website down. Rackspace instance down? Website down. MySQL down\u2026 you get the idea.</p>\n</div>\n<div class=\"paragraph\">\n<p>Another aspect of it was its cost.</p>\n</div>\n<div class=\"paragraph\">\n<p>Our average monthly bill quickly climbed over $1,000. Which was quite a price tag for a single machine and the\u2014\u200blow\u2014\u200bamount of traffic we generated at the time.</p>\n</div>\n<div class=\"paragraph\">\n<p>After a couple years running this stack, mid-2013, I decided it was time to make our website more scalable, redundant, but also more cost effective.</p>\n</div>\n<div class=\"paragraph\">\n<p>I estimated, we needed a minimum of 3 servers to make our website somewhat redundant which would amount to a whopping $14400/year at Rackspace. Being a really small startup, we couldn\u2019t justify that \u201chigh\u201d of an infrastructure bill; I kept looking.</p>\n</div>\n<div class=\"paragraph\">\n<p>The cheapest option ended up to be running our stack on bare-metal servers.</p>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_ovh\">OVH</h3>\n<div class=\"paragraph\">\n<p>I had worked in the past with OVH and had always been fairly satisfied (despite mixed reviews online). I estimated that running 3 servers at OVH would amount to $3240/year, almost 5 times less expensive than Rackspace.</p>\n</div>\n<div class=\"paragraph\">\n<p>Not only was OVH cheaper, but their servers were also 4 times more powerful than Rackspace\u2019s: 32GB RAM, 8 CPUs, SSDs and unlimited bandwidth.</p>\n</div>\n<div class=\"paragraph\">\n<p>To top it off they had just opened a new datacenter in North America.</p>\n</div>\n<div class=\"paragraph\">\n<p>A few weeks later Betabrand.com was hosted at OVH in Beauharnois, Canada.</p>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"_hardware_infrastructure\">Hardware infrastructure</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph\">\n<p>Between 2013 and 2017, our hardware infrastructure went through a few architectural changes.</p>\n</div>\n<div class=\"paragraph\">\n<p>Towards the end of 2017, our stack was significantly larger than it used to be, both in terms of software and hardware.</p>\n</div>\n<div class=\"paragraph\">\n<p>Betabrand.com ran on 17 bare-metal servers:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>2 HAProxy machines in charge of SSL Offloading configured as hot-standby</li>\n<li>2 varnish-cache machines configured in a hot-standby load-balancing to our webservers</li>\n<li>5 machines running Apache and PHP-FPM</li>\n<li>2 redis servers, each running 2 separate instances of redis. 1 instance for some application caching, 1 instance for our PHP sessions</li>\n<li>3 MariaDB servers configured as master-master, though used in a master-slave manner</li>\n<li>3 Glusterd servers serving all our static assets</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>Each machine would otherwise run one or multiple processes like keepalived, Ganglia, Munin, logstash, exim, backup-manager, supervisord, sshd, fail2ban, prerender, rabbitmq and\u2026 docker.</p>\n</div>\n<div class=\"paragraph\">\n<p>However, while this infrastructure was very cheap, redundant and had no single point of failure, it still wasn\u2019t scalable and was also much harder to maintain.</p>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_the_scalability_and_maintainability_issue\">The scalability and maintainability issue</h3>\n<div class=\"paragraph\">\n<p>Administering our server \u201cfleet\u201d now involved writing a set of Ansible scripts and maintaining them, which, despite Ansible being an amazing software, was no easy feat.</p>\n</div>\n<div class=\"paragraph\">\n<p>Even though it will make its best effort to get you there, Ansible doesn\u2019t guarantee the state of your system.</p>\n</div>\n<div class=\"paragraph\">\n<p>For example, running your Ansible scripts on a server fleet made of heterogeneous OSes (say debian 8 and debian 9) will bring all your machines to a state close to what you defined, but you will most likely end up with discrepancies; the first one being that you\u2019re running on Debian 8 and Debian 9, but also software versions and configurations being different on some servers and others.</p>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tbody>\n<tr>\n<td class=\"icon\">\n<div class=\"title\"></div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>I searched quite often for an Ansible replacement, but never found better.</p>\n</div>\n<div class=\"paragraph\">\n<p>I looked into Puppet but found its learning curve too steep, and, from reading other people\u2019s recipes, was taken aback by what\u00a0<em>seemed</em>\u00a0to be too many different ways of doing the same thing. Some people might think of this as flexibility, I see it as complexity.</p>\n</div>\n<div class=\"paragraph\">\n<p>SaltStack caught my eyes but also found it very hard to learn; despite their extensive, in depth documentation, their nomenclature choices (mine, pillar, salt, etc) never stuck with me; and it seemed to suffer the same issue as Puppet regarding complexity.</p>\n</div>\n<div class=\"paragraph\">\n<p>Nix package manager and NixOS sounded amazing, to the exception that I didn\u2019t feel comfortable learning a whole new OS (I\u2019ve been using Debian for years) and was worried that despite their huge package selection, I would eventually need packages not already available, which would then become something new to maintain.</p>\n</div>\n<div class=\"paragraph\">\n<p>Those are the only 3 I looked at but I\u2019m sure there\u2019s many other tools out there I\u2019ve probably never heard of.</p>\n</div>\n</td>\n</tr>\n</tbody>\n</table>\n</div>\n<div class=\"paragraph\">\n<p>Writing Ansible scripts and maintaining them, however, wasn\u2019t our only issue; adding capacity was another one.</p>\n</div>\n<div class=\"paragraph\">\n<p>With bare-metal, it is impossible to add and remove capacity on the fly. You need to plan your needs well in advance: buy a machine\u2014\u200busually leased for a minimum of 1 month\u2014\u200bwait for it to be ready\u2014\u200bwhich can take from 2 minutes to 3 days-, install its base os, install Ansible\u2019s dependencies (mainly python and a few other packages) then, finally, run your Ansible scripts against it.</p>\n</div>\n<div class=\"paragraph\">\n<p>For us this entire process was wholly unpractical and what usually happened is that we\u2019d add capacity for an anticipated peak load, but never would remove it afterwards which in turn added to our costs.</p>\n</div>\n<div class=\"paragraph\">\n<p>It is worth noting, however, that even though having unused capacity in your infrastructure is akin to setting cash on fire, it is still a magnitude less expensive on bare-metal than in the cloud. On the other hand, the engineering headaches that come with using bare-metal servers simply shift the cost from purely material to administrative ones.</p>\n</div>\n<div class=\"paragraph\">\n<p>In our bare-metal setup capacity planning, server administration and Ansible scripting were just the tip of the iceberb.</p>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_scaling_development_processes\">Scaling development processes</h3>\n<div class=\"paragraph\">\n<p>In early 2017, while our infrastructure had grown, so had our team.</p>\n</div>\n<div class=\"paragraph\">\n<p>We hired 7 more engineers making us a small 9 people team, with skillsets distributed all over the spectrum from backend to frontend with varying levels of seniority.</p>\n</div>\n<div class=\"paragraph\">\n<p>Even in a small 9 people team, being productive and limiting the amount of bugs deployed to production warrants a simple, easy to setup and easy to use development-staging-production trifecta.</p>\n</div>\n<div class=\"paragraph\">\n<p>Setting up your development environment as a new hire shouldn\u2019t take hours, neither should upgrading or re-creating it.</p>\n</div>\n<div class=\"paragraph\">\n<p>Moreover, a company-wide accessible staging environment should exist and match 99% of your production, if not 100%.</p>\n</div>\n<div class=\"paragraph\">\n<p>Unfortunately, in our hardware infrastructure reaching this harmonious trifecta was impossible.</p>\n</div>\n<div class=\"sect3\">\n<h4 id=\"_the_development_environment\">The development environment</h4>\n<div class=\"paragraph\">\n<p>First of all, everybody in our engineering team uses MacBook Pros, which is an issue since our stack is linux based.</p>\n</div>\n<div class=\"paragraph\">\n<p>However, asking everybody to switch to linux and potentially change their precious workflow wasn\u2019t really ideal. This meant that the best solution was to provide a development environment agnostic of developers&#8217; personal preferences in machines.</p>\n</div>\n<div class=\"paragraph\">\n<p>I could only see two obvious options:</p>\n</div>\n<div class=\"paragraph\">\n<p>Either provide a Vagrant stack that would run multiple virtual machines (17 potentially, though, more realistically, 1 machine running our entire stack), or, re-use the already written ansible scripts and run them against our local macbooks.</p>\n</div>\n<div class=\"paragraph\">\n<p>After investigating Vagrant, I felt that using virtual machines would hinder performances too much and wasn\u2019t worth it. I decided, for better or worse, to go the Ansible route (in hindsight, this probably wasn\u2019t the best decision).</p>\n</div>\n<div class=\"paragraph\">\n<p>We would use the same set of Ansible scripts on production, staging and dev. The caveat being of course that our development stack, although close to production, was not a 100% match.</p>\n</div>\n<div class=\"paragraph\">\n<p>This worked well enough for a while; However, the mismatch caused issues later when, for example, our development and production MySQL versions weren\u2019t aligned. Some queries that ran on dev, wouldn\u2019t on production.</p>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"_the_staging_environment\">The staging environment</h4>\n<div class=\"paragraph\">\n<p>Secondly, having a development and production environments running on widely different softwares (mac os versus debian) meant that we absolutely needed a staging environment.</p>\n</div>\n<div class=\"paragraph\">\n<p>Not only because of potential bugs caused by version mismatches, but also because we needed a way to share new features to external members before launch.</p>\n</div>\n<div class=\"paragraph\">\n<p>Once again I had multiple choices:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>buy 17 servers and run ansible against them. This would double our costs though and we were trying to save money.</li>\n<li>setup our entire stack on a unique linux server, accessible from the outside. Cheaper solution, but once again not providing an exact replica of our production system.</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>I decided to implement the cost-saving solution.</p>\n</div>\n<div class=\"paragraph\">\n<p>An early version of the staging environment involved 3 independant linux servers, each running the entire stack. Developers would then yell across the room (or hipchat) \u201ctaking over dev1\u201d, \u201cis anybody using dev3?\u201d, \u201cdev2 is down\u00a0:/\u201d.</p>\n</div>\n<div class=\"paragraph\">\n<p>Overall, our development-staging-production setup was far from optimal: it did the job; but definitely needed improvements.</p>\n</div>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"_the_advent_ofdocker\">The advent of\u00a0Docker</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph\">\n<p>In 2013 Dotcloud released Docker.</p>\n</div>\n<div class=\"paragraph\">\n<p>The Betabrand use case for Docker was immediately obvious. I saw it as the solution to simplify our development and staging environments; by getting rid of the ansible scripts (well, almost; more on that later).</p>\n</div>\n<div class=\"paragraph\">\n<p>Those scripts would now only be used for production.</p>\n</div>\n<div class=\"paragraph\">\n<p>At the time, one main pain point for the team was competing for our three physical staging servers: dev1, dev2 and dev3; and for me maintaining those 3 servers was a major annoyance.</p>\n</div>\n<div class=\"paragraph\">\n<p>After observing docker for a few months, I decided to give it a go in April 2014.</p>\n</div>\n<div class=\"paragraph\">\n<p>After installing docker on one of the staging servers, I created a single docker image containing our entire stack (haproxy, varnish, redis, apache, etc.) then over the next few months wrote a tool (<code>sailor</code>) allowing us to create, destroy and manage an infinite number of staging environment accessible via individual unique URLs.</p>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tbody>\n<tr>\n<td class=\"icon\">\n<div class=\"title\"></div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>Worth noting that docker-compose didn\u2019t exist at that time; and that putting your entire stack inside one docker image is of course a big no-no but that\u2019s an unimportant detail here.</p>\n</div>\n</td>\n</tr>\n</tbody>\n</table>\n</div>\n<div class=\"paragraph\">\n<p>From this point on, the team wasn\u2019t competing anymore for access to the staging servers. Anybody could create a new, fully configured, staging container from the docker image using sailor. I didn\u2019t need to maintain the servers anymore either; better yet, I shut down and cancelled 2 of them.</p>\n</div>\n<div class=\"paragraph\">\n<p>Our development environment, however, still was running on macos (well, \u201cMac OS X\u201d at the time) and using the Ansible scripts.</p>\n</div>\n<div class=\"paragraph\">\n<p>Then, sometime around 2016 docker-machine was released.</p>\n</div>\n<div class=\"paragraph\">\n<p>Docker machine is a tool taking care of deploying a docker daemon on any stack of your choice: virtualbox, aws, gce, bare-metal, azure, you name it, docker-machine does it; in one command line.</p>\n</div>\n<div class=\"paragraph\">\n<p>I saw it as the opportunity to easily and quickly migrate our ansible-based development environment to a docker based one. I modified\u00a0<code>sailor</code>\u00a0to use docker-machine as its backend.</p>\n</div>\n<div class=\"paragraph\">\n<p>Setting up a development environment was now a matter of creating a new docker-machine then passing a flag for sailor to use it.</p>\n</div>\n<div class=\"paragraph\">\n<p>At this point, our development-staging process had been simplified tremendously; at least from a dev-ops perspective: anytime I needed to upgrade any software of our stack to a newer version or change the configuration, instead of modifying my ansible scripts, asking all the team to run them, then running them myself on all 3 staging servers; I could now simply push a new docker image.</p>\n</div>\n<div class=\"paragraph\">\n<p>Ironically enough, I ended up needing virtual machines (which I had deliberately avoided) to run docker on our macbooks. Using vagrant instead of Ansible would have been a better choice from the get go. Hindsight is always 20/20.</p>\n</div>\n<div class=\"paragraph\">\n<p>Using docker for our development and staging systems paved the way to the better solution that Betabrand.com now runs on.</p>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"_kubernetes\">Kubernetes</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph\">\n<p>Because Betabrand is primarily an e-commerce platform, Black Friday loomed over our website more and more each year.</p>\n</div>\n<div class=\"paragraph\">\n<p>To our surprise, the website had handled increasingly higher loads since 2013 without failing in any major catastrophe, but, it did require a month long preparation beforehand: adding capacity, load testing and optimizing our checkout code paths as much as we possibly could.</p>\n</div>\n<div class=\"paragraph\">\n<p>After preparing for Black Friday 2016, however, it became evident the infrastructure wouldn\u2019t scale for Black Friday 2017; I worried the website would become inacessible under the load.</p>\n</div>\n<div class=\"paragraph\">\n<p>Luckily, sometime in 2015, the release of Kubernetes 1.0 caught my attention.</p>\n</div>\n<div class=\"paragraph\">\n<p>Just like I saw in docker an obvious use-case, I knew k8s was what we needed to solve many of our issues. First of all, it would finally allow us to run an\u00a0<em>almost</em>\u00a0identical dev-staging-production environment. But also, would solve our scalability issues.</p>\n</div>\n<div class=\"paragraph\">\n<p>I also evaluated 2 other solutions, Nomad and Docker Swarm, but Kubernetes seemed to be the most promising.</p>\n</div>\n<div class=\"paragraph\">\n<p>For Black Friday 2017, I set out to migrate our entire infra to k8s.</p>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tbody>\n<tr>\n<td class=\"icon\">\n<div class=\"title\"></div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>Although I considered it, I quickly ruled out using our current OVH bare-metal servers for our k8s nodes since it would play against my goal of getting rid of Ansible and not dealing with all the issue that comes with hardware servers. Moreover, soon after I started investigating Kubernetes, Google released their managed Kubernetes (GKE) offer, which I rapidly came to choose.</p>\n</div>\n</td>\n</tr>\n</tbody>\n</table>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_learning_kubernetes\">Learning Kubernetes</h3>\n<div class=\"paragraph\">\n<p>Migrating to k8s first involved gaining a strong understanding its architecture and its concepts, by reading the online documentation.</p>\n</div>\n<div class=\"paragraph\">\n<p>Most importantly understanding containers, Pods, Deployments and Services and how they all fit together. Then in order, ConfigMaps, Secrets, Daemonsets, StatefulSets, Volumes, PersistentVolumes and PersistentVolumeClaims.</p>\n</div>\n<div class=\"paragraph\">\n<p>Other concepts are important, though less necessary to get a cluster going.</p>\n</div>\n<div class=\"paragraph\">\n<p>Once I assimilated those concepts, the second, and hardest, step involved translating our bare-metal architecture into a set of YAML manifests.</p>\n</div>\n<div class=\"paragraph\">\n<p>From the beginning I set out to have one, and only one, set of manifests to be used for the creation of all three development, staging and production environment. I quickly ran into needing to parameterized my YAML manifests, which isn\u2019t out-of-the-box supported by Kubernetes. This is where Helm\u00a0<sup class=\"footnote\">[<a id=\"_footnoteref_1\" class=\"footnote\" title=\"View footnote.\" href=\"https://boxunix.com/post/bare_metal_to_kube/#_footnotedef_1\">1</a>]</sup>\u00a0comes in handy.</p>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tbody>\n<tr>\n<td class=\"icon\">\n<div class=\"title\"></div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p><em>from the Helm website</em>: Helm helps you manage Kubernetes applications\u2014\u200bHelm Charts helps you define, install, and upgrade even the most complex Kubernetes application.</p>\n</div>\n</td>\n</tr>\n</tbody>\n</table>\n</div>\n<div class=\"paragraph\">\n<p>Helm markets itself as a package manager for Kubernetes, I originally used it solely for its templating feature though. I have, now, also come to appreciate its package manager aspect and used it to install Grafana\u00a0<sup class=\"footnote\">[<a id=\"_footnoteref_2\" class=\"footnote\" title=\"View footnote.\" href=\"https://boxunix.com/post/bare_metal_to_kube/#_footnotedef_2\">2</a>]</sup>\u00a0and Prometheus\u00a0<sup class=\"footnote\">[<a id=\"_footnoteref_3\" class=\"footnote\" title=\"View footnote.\" href=\"https://boxunix.com/post/bare_metal_to_kube/#_footnotedef_3\">3</a>]</sup>.</p>\n</div>\n<div class=\"paragraph\">\n<p>After a bit of sweat and a few tears, our infrastructure was now neatly organized into 1 Helm package, 17 Deployments, 9 ConfigMaps, 5 PersistentVolumeClaims, 5 Secrets, 18 Services, 1 StatefulSet, 2 StorageClasses, 22 container images.</p>\n</div>\n<div class=\"paragraph\">\n<p>All that was left was to migrate to this new infrastructure and shutdown all our hardware servers.</p>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_officially_migrating\">Officially migrating</h3>\n<div class=\"paragraph\">\n<p>October 5th 2017 was the night.</p>\n</div>\n<div class=\"paragraph\">\n<p>Pulling the trigger was extremely easy and went without a hitch.</p>\n</div>\n<div class=\"paragraph\">\n<p>I created a new GKE cluster, ran\u00a0<code>helm install betabrand --name production</code>, imported our MySQL database to Google Cloud SQL, then, after what actually took about 2 hours, we were live in the Clouds.</p>\n</div>\n<div class=\"paragraph\">\n<p>The migration was\u00a0<em>that</em>\u00a0simple.</p>\n</div>\n<div class=\"paragraph\">\n<p>What helped a lot of course, was the ability to create multiple clusters in Google GKE: before migrating our production, I was able to rehearse through many test migration, jotting down every steps needed for a successful launch.</p>\n</div>\n<div class=\"paragraph\">\n<p>Black Friday 2017 was very successful for Betabrand and the few technical issues we ran into weren\u2019t associated to the migration.</p>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_the_developmentstaging_environments\">The development/staging environments</h3>\n<div class=\"paragraph\">\n<p>Our development machines run a Kubernetes cluster via Minikube\u00a0<sup class=\"footnote\">[<a id=\"_footnoteref_4\" class=\"footnote\" title=\"View footnote.\" href=\"https://boxunix.com/post/bare_metal_to_kube/#_footnotedef_4\">4</a>]</sup>.</p>\n</div>\n<div class=\"paragraph\">\n<p>The same YAML manifests are being used to create a local development environment or a \u201cproduction-like\u201d environment.</p>\n</div>\n<div class=\"paragraph\">\n<p>Everything that runs on Production, also runs in Development. The only difference between the two environments is that our development environment talks to a local MySQL database, whereas production talks to Google Cloud SQL.</p>\n</div>\n<div class=\"paragraph\">\n<p>Creating a staging environment is exactly the same as creating a new production cluster: all that is needed is to clone the production database instance (which is only a few clicks or one command line) then point the staging cluster to this database via a\u00a0<code>--set database</code>parameter in\u00a0<code>helm</code>.</p>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_a_year_after\">A year after</h3>\n<div class=\"paragraph\">\n<p>It\u2019s now been a year and 2 months since we moved our infrastructure to Kubernetes and I couldn\u2019t be happier.</p>\n</div>\n<div class=\"paragraph\">\n<p>Kubernetes has been rock solid in production and we have yet to experience an outage.</p>\n</div>\n<div class=\"paragraph\">\n<p>In anticipation of a lot of traffic for Black Friday 2018, we were able to create an exact replica of our production services in a few minutes and do a lot of load testing. Those load tests revealed specific code paths performing extremely poorly that only a lot of traffic could reveal and allowed us to fix them before Black Friday.</p>\n</div>\n<div class=\"paragraph\">\n<p>As expected, Black Friday 2018 brought more traffic than ever to Betabrand.com, but k8s met its promises, and, features like the HorizontalPodAutoscaler coupled to GKE\u2019s node autoscaling allowed our website to absorb peak loads without any issues.</p>\n</div>\n<div class=\"paragraph\">\n<p>K8s, combined with GKE, gave us the tools we needed to make our infrastructure reliable, available, scalable and maintainable.</p>\n</div>\n</div>\n</div>\n</div>\n<div id=\"footnotes\">\n<hr />\n<div id=\"_footnotedef_1\" class=\"footnote\"><a href=\"https://boxunix.com/post/bare_metal_to_kube/#_footnoteref_1\">1</a>.\u00a0<a class=\"bare\" href=\"https://helm.sh/\">https://helm.sh/</a></div>\n<div id=\"_footnotedef_2\" class=\"footnote\"><a href=\"https://boxunix.com/post/bare_metal_to_kube/#_footnoteref_2\">2</a>.\u00a0<a class=\"bare\" href=\"https://grafana.com/\">https://grafana.com/</a></div>\n<div id=\"_footnotedef_3\" class=\"footnote\"><a href=\"https://boxunix.com/post/bare_metal_to_kube/#_footnoteref_3\">3</a>.\u00a0<a class=\"bare\" href=\"https://prometheus.io/\">https://prometheus.io/</a></div>\n<div id=\"_footnotedef_4\" class=\"footnote\"><a href=\"https://boxunix.com/post/bare_metal_to_kube/#_footnoteref_4\">4</a>.\u00a0<a class=\"bare\" href=\"https://github.com/kubernetes/minikube\">https://github.com/kubernetes/minikube</a></div>\n</div>\n", "slug": "from-bare-metal-to-kubernetes", "date": 1559684834, "cats": [7, 0, 8, 1, 4]}